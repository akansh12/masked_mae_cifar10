{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block, PatchEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(torch.nn.Module):\n",
    "    def __init__(self, patch_size, emb_dim, in_chans = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.in_chans = in_chans\n",
    "        self.proj = nn.Conv2d(in_chans, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c') # Final shape is (#batches, #patches, #emb_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n",
      "torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "#test Patchify\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "patchify = Patchify(16, 768)\n",
    "patches = patchify(x)\n",
    "print(patches.shape)\n",
    "\n",
    "model = PatchEmbed(\n",
    "    img_size=224, patch_size=16, in_chans=3, embed_dim=768\n",
    ")\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "patches = model(x)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size = 32,\n",
    "            patch_size = 2,\n",
    "            in_chans = 3,\n",
    "            emb_dim = 192,\n",
    "            num_layers = 12,\n",
    "            num_heads = 3,\n",
    "            mask_ratio = 0.75,\n",
    "            mlp_dim = 768\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding  = nn.Parameter(torch.empty(1, (img_size // patch_size) ** 2 + 1, emb_dim))\n",
    "\n",
    "        self.patchify = PatchEmbed(\n",
    "            img_size = img_size,\n",
    "            patch_size = patch_size,\n",
    "            in_chans = in_chans,\n",
    "            embed_dim = emb_dim\n",
    "        )\n",
    "        ### Encoder model\n",
    "        self.encoder = nn.ModuleList([\n",
    "            Block(\n",
    "                dim = emb_dim,\n",
    "                num_heads = num_heads,\n",
    "                mlp_ratio = mlp_dim / emb_dim,\n",
    "                qkv_bias = True,\n",
    "                norm_layer = nn.LayerNorm,\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm_layer = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT: #Code taken from FAIR\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "\n",
    "    def forward(self, x, mask_ratio = 0.75):\n",
    "        x = self.patchify(x)\n",
    "\n",
    "        #Add position embedding w/o cls token\n",
    "        x = x + self.pos_embedding[:, 1:, :]\n",
    "\n",
    "        #masking\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        #Append cls token\n",
    "        cls_token = self.cls_token + self.pos_embedding[:, 0:1, :]\n",
    "        cls_token = cls_token.expand(x.shape[0], -1, -1) #Expand cls token to all batches\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        for block in self.encoder:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm_layer(x)\n",
    "\n",
    "        return x, mask, ids_restore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_size = 32,\n",
    "        patch_size = 2,\n",
    "        emb_dim = 192,\n",
    "        num_layers = 4,\n",
    "        num_heads = 3,\n",
    "        out_chans = 3,\n",
    "        mlp_dim = 768\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.decoder_pos_embedding = nn.Parameter(torch.empty(1, (image_size // patch_size) ** 2 + 1, emb_dim))\n",
    "\n",
    "        # self.decoder_emb = nn.Linear(encoder_emb_dim, decoder_embed_dim, bias=True)\n",
    "        self.decoder = nn.ModuleList([\n",
    "            Block(\n",
    "                dim = emb_dim,\n",
    "                num_heads = num_heads,\n",
    "                mlp_ratio = mlp_dim / emb_dim,\n",
    "                qkv_bias = True,\n",
    "                norm_layer = nn.LayerNorm,\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(emb_dim)\n",
    "        self.decoder_pred = nn.Linear(emb_dim, patch_size **2 * out_chans, bias=True)\n",
    "        self.patch2img = Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size // patch_size, w=image_size // patch_size)\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        torch.nn.init.normal_(self.decoder_pos_embedding, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT: #Code taken from FAIR\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, ids_restore):\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        #add position embedding\n",
    "        x = x + self.decoder_pos_embedding\n",
    "\n",
    "        for block in self.decoder:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        #remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 192])\n",
      "torch.Size([4, 65, 192])\n",
      "torch.Size([4, 256, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test MAE_Encoder and MAE_Decoder\n",
    "encoder = MAE_Encoder()\n",
    "decoder = MAE_Decoder()\n",
    "\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "x, mask, ids_restore = encoder(x)\n",
    "print(x.shape)\n",
    "x = decoder(x, ids_restore)\n",
    "print(x.shape)\n",
    "decoder.patch2img(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size = 32,\n",
    "        patch_size = 2,\n",
    "        in_chans = 3,\n",
    "        encoder_emb_dim = 192,\n",
    "        encoder_layers = 12,\n",
    "        encoder_heads = 3,\n",
    "        encoder_mlp_dim = 768,\n",
    "        decoder_layers = 4,\n",
    "        decoder_heads = 3,\n",
    "        decoder_mlp_dim = 768,\n",
    "        out_chans = 3\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(\n",
    "            img_size = img_size,\n",
    "            patch_size = patch_size,\n",
    "            in_chans = in_chans,\n",
    "            emb_dim = encoder_emb_dim,\n",
    "            num_layers = encoder_layers,\n",
    "            num_heads = encoder_heads,\n",
    "            mlp_dim = encoder_mlp_dim\n",
    "        )\n",
    "\n",
    "        self.decoder = MAE_Decoder(\n",
    "            image_size = img_size,\n",
    "            patch_size = patch_size,\n",
    "            emb_dim = encoder_emb_dim,\n",
    "            num_layers = decoder_layers,\n",
    "            num_heads = decoder_heads,\n",
    "            mlp_dim = decoder_mlp_dim,\n",
    "            out_chans = out_chans\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, mask, ids_restore = self.encoder(x)\n",
    "        x = self.decoder(x, ids_restore)\n",
    "        return x, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss_function(input_img, pred_img, mask, patch_size=2):\n",
    "    \"\"\"\n",
    "    input_img: [B, C, H, W]\n",
    "    pred_img: [B, npatch, patch_size*patch_size*C]\n",
    "    mask: [B, npatch]    \n",
    "    \"\"\"\n",
    "    #reshape input_img\n",
    "    input_img = rearrange(input_img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size)\n",
    "\n",
    "    loss = (input_img - pred_img) ** 2\n",
    "    print(loss.shape)\n",
    "    loss = loss.mean(dim=-1)  \n",
    "    print(loss.shape)\n",
    "\n",
    "    loss = (loss * mask).sum() / mask.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 192])\n",
      "torch.Size([1, 256, 12]) torch.Size([1, 256])\n",
      "torch.Size([1, 256, 12])\n",
      "torch.Size([1, 256, 12])\n",
      "torch.Size([1, 256])\n",
      "tensor(2.4931, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test MAE\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "mae = MAE()\n",
    "pred, mask = mae(x)\n",
    "print(pred.shape, mask.shape)\n",
    "loss = mae_loss_function(x, pred, mask)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT_Classifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            img_size = 32,\n",
    "            patch_size = 2,\n",
    "            in_chans = 3,\n",
    "            num_classes = 10,\n",
    "            emb_dim = 192,\n",
    "            num_layers = 12,\n",
    "            num_heads = 3,\n",
    "            mlp_dim = 768,\n",
    "            pretrained = False,\n",
    "            pretrained_path = None\n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.encoder_model = MAE_Encoder(\n",
    "            img_size = img_size,\n",
    "            patch_size = patch_size,\n",
    "            in_chans = in_chans,\n",
    "            emb_dim = emb_dim,\n",
    "            num_layers = num_layers,\n",
    "            num_heads = num_heads,\n",
    "            mlp_dim = mlp_dim\n",
    "        )\n",
    "        self.cls_token = self.encoder_model.cls_token\n",
    "        self.pos_embedding = self.encoder_model.pos_embedding\n",
    "        self.patchify = self.encoder_model.patchify\n",
    "        self.encoder = self.encoder_model.encoder\n",
    "        self.norm_layer = self.encoder_model.norm_layer\n",
    "\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "        if pretrained:\n",
    "            self.load_state_dict(torch.load(pretrained_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patchify(x)\n",
    "\n",
    "        #Append cls token\n",
    "\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        #Add position embedding\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        for block in self.encoder:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm_layer(x)\n",
    "\n",
    "        x = x[:, 0, :] #cls token\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "#test VIT_Classifier\n",
    "x = torch.randn(4, 3, 32, 32)\n",
    "model = VIT_Classifier()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
